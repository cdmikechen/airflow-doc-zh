# 使用Mesos扩展（社区贡献）

> 译者：[@ImPerat0R\_](https://github.com/tssujt)、[@ThinkingChen](https://github.com/cdmikechen)

有两种方法可以将Airflow作为mesos框架运行：

1. 想要直接在mesos slaves上运行Airflow任务，要求每个mesos slave安装和配置Airflow。
2. 在安装了Airflow的docker容器内运行Airflow任务，该容器在mesos slave上运行。

## 任务直接在mesos slave上执行

`MesosExecutor`允许您在Mesos群集上调度Airflow任务。为此，您需要一个正在运行的mesos集群，并且必须执行以下步骤 -

1. 在可以运行web server和scheduler的mesos slave上安装Airflow，让我们将其称为“Airflow server”。
2. 在 Airflow server 上，从[mesos下载](http://open.mesosphere.com/downloads/mesos/)安装mesos python eggs。
3. 在 Airflow server 上，使用可以被所有mesos slave访问的数据库（例如mysql），并在`airflow.cfg`添加配置。
4. 将您的`airflow.cfg`里 executor 的参数指定为*MesosExecutor*，并提供相关的Mesos设置。
5. 在所有mesos slave上，安装Airflow。 从Airflow服务器复制`airflow.cfg` （以便它使用相同的sqlalchemy连接）。
6. 在所有mesos slave上，运行以下服务日志：

```py
airflow serve_logs
```

7. 在 Airflow server 上，如果想要开始在mesos上处理/调度DAG，请运行：

```py
airflow scheduler -p
```

注意：我们需要-p参数来挑选DAG。

您现在可以在mesos UI中查看Airflow框架和相应的任务。Airflow任务的日志可以像往常一样在Airflow UI中查看。

有关mesos的更多信息，请参阅[mesos文档](http://mesos.apache.org/documentation/latest/)。 有关MesosExecutor的任何疑问/错误，请联系[@kapil-malik](https://github.com/kapil-malik)。

## 在mesos slave上的容器中执行的任务

[此gist](https://gist.github.com/sebradloff/f158874e615bda0005c6f4577b20036e)包含实现以下所需的所有文件和配置更改：

1. 使用安装了mesos python eggs的软件环境创建一个dockerized版本的Airflow。

> 我们建议利用docker的多阶段构建来实现这一目标。我们有一个Dockerfile定义从源（Dockerfile-mesos）构建特定版本的mesos，以便创建python eggs。在Airflow Dockerfile（Dockerfile-airflow）中，我们从mesos镜像中复制python eggs。

2. 在`airflow.cfg`创建一个mesos配置块。

> 配置块保持与默认Airflow配置（default_airflow.cfg）相同，但添加了一个选项`docker_image_slave`。 这应该设置为您希望mesos在运行Airflow任务时使用的镜像的名称。确保您具有适用于您的mesos主服务器的DNS记录的正确配置以及任何类型的授权（如果存在）。

3. 更改`airflow.cfg`以将执行程序参数指向MesosExecutor（executor = SequentialExecutor）。

4. 确保您的mesos slave可以访问您`docker_image_slave`的docker存储库。

> [mesos文档中提供了相关说明。](https://mesos.readthedocs.io/en/latest/docker-containerizer/)

其余部分取决于您以及您希望如何使用dockerized Airflow配置。
